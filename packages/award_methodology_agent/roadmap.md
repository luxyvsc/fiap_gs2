# Award Methodology Agent - Roadmap

## üèÜ Vis√£o Geral

Agente de IA que gera metodologias objetivas e transparentes para premia√ß√µes, competi√ß√µes e rankings acad√™micos.

### Responsabilidades
- Criar crit√©rios claros de premia√ß√£o
- Avaliar trabalhos competitivos
- Gerar ranking transparente com justificativas
- Explicar decis√µes de forma compreens√≠vel
- Mostrar o que faltou para posi√ß√µes superiores

---

## üéØ Funcionalidades

### 1. Gera√ß√£o de Metodologia de Premia√ß√£o
- An√°lise de objetivos da competi√ß√£o
- Defini√ß√£o de crit√©rios mensur√°veis
- Pesos por crit√©rio
- Regras de desempate
- Valida√ß√£o de fairness (sem vi√©s)

### 2. Avalia√ß√£o Competitiva
- An√°lise comparativa entre trabalhos
- Aplica√ß√£o de crit√©rios
- C√°lculo de pontua√ß√£o
- Ranking autom√°tico

### 3. Transpar√™ncia e Explicabilidade
- Justificativa clara para cada pontua√ß√£o
- Compara√ß√£o entre trabalhos
- "O que faltou" para posi√ß√£o superior
- Visualiza√ß√µes de scores

### 4. Interface de Aprova√ß√£o
- Visualiza√ß√£o de ranking gerado
- Edi√ß√£o de pontua√ß√µes
- Conversa com agente para ajustes
- Aprova√ß√£o final

---

## üìã Tarefas de Implementa√ß√£o

### Fase 1: Gera√ß√£o de Metodologia
- [ ] Parser de regras de competi√ß√£o
- [ ] Agente CrewAI para criar crit√©rios
- [ ] Exemplo de metodologia:
  ```
  Competi√ß√£o: Hackathon FIAP 2025
  
  Crit√©rios:
  1. Inova√ß√£o (30%)
     - Originalidade da ideia (0-10)
     - Viabilidade t√©cnica (0-10)
  
  2. Execu√ß√£o T√©cnica (30%)
     - Qualidade do c√≥digo (0-10)
     - Arquitetura (0-10)
  
  3. Apresenta√ß√£o (20%)
     - Clareza da explica√ß√£o (0-10)
     - Qualidade do v√≠deo (0-10)
  
  4. Impacto (20%)
     - Relev√¢ncia do problema (0-10)
     - Potencial de uso real (0-10)
  
  Desempate: Maior nota em Inova√ß√£o
  ```

### Fase 2: Avalia√ß√£o Automatizada
- [ ] An√°lise de cada trabalho conforme crit√©rios
- [ ] Pontua√ß√£o automatizada onde poss√≠vel
- [ ] Compara√ß√£o relativa entre trabalhos
- [ ] Detec√ß√£o de outliers (excepcional ou fraco)

### Fase 3: Gera√ß√£o de Ranking e Justificativas
- [ ] C√°lculo de pontua√ß√£o total
- [ ] Ordena√ß√£o dos trabalhos
- [ ] Gera√ß√£o de justificativas:
  ```markdown
  ## Ranking Final - Hackathon FIAP 2025
  
  ### ü•á 1¬∫ Lugar - Grupo Alpha (91.5 pontos)
  **Por que ganhou:**
  - Inova√ß√£o excepcional (28/30): Solu√ß√£o √∫nica que ningu√©m mais pensou
  - Execu√ß√£o t√©cnica s√≥lida (26/30): C√≥digo limpo e bem arquitetado
  - Apresenta√ß√£o clara (18/20): V√≠deo profissional e bem explicado
  - Impacto relevante (19/20): Problema real com solu√ß√£o vi√°vel
  
  ### ü•à 2¬∫ Lugar - Grupo Beta (88.0 pontos)
  **Pontos fortes:**
  - Execu√ß√£o t√©cnica excelente (29/30): Melhor c√≥digo da competi√ß√£o
  - Apresenta√ß√£o impec√°vel (20/20): Melhor apresenta√ß√£o
  
  **O que faltou para o 1¬∫ lugar:**
  - Inova√ß√£o (24/30 vs 28/30): Ideia boa mas n√£o t√£o original
  - Diferen√ßa total: 3.5 pontos
  
  ### ü•â 3¬∫ Lugar - Grupo Gamma (85.5 pontos)
  **Pontos fortes:**
  - Inova√ß√£o criativa (27/30)
  - Impacto social importante (18/20)
  
  **O que faltou para o 2¬∫ lugar:**
  - Execu√ß√£o t√©cnica (21/30): C√≥digo com v√°rias melhorias poss√≠veis
  - Apresenta√ß√£o (16/20): V√≠deo poderia ser mais claro
  - Diferen√ßa total: 2.5 pontos
  ```

### Fase 4: Visualiza√ß√µes
- [ ] Gr√°fico radar por crit√©rio
- [ ] Heatmap de compara√ß√£o entre grupos
- [ ] Timeline de pontua√ß√£o (se m√∫ltiplas etapas)
- [ ] Boxplot de distribui√ß√£o de notas

### Fase 5: Interface de Aprova√ß√£o
- [ ] Dashboard com ranking proposto
- [ ] Drill-down em cada grupo
- [ ] Edi√ß√£o de pontua√ß√µes
- [ ] Chat com agente para ajustes
- [ ] Aprova√ß√£o e publica√ß√£o

### Fase 6: Testes e Deploy
- [ ] Valida√ß√£o com competi√ß√µes anteriores
- [ ] Teste de fairness (sem vi√©s)
- [ ] Deploy serverless

---

## üîå Endpoints

- `POST /api/v1/awards/methodology/generate` - Gerar metodologia
- `POST /api/v1/awards/evaluate` - Avaliar competi√ß√£o
- `GET /api/v1/awards/ranking/{competition_id}` - Ver ranking
- `GET /api/v1/awards/explanation/{group_id}` - Explica√ß√£o detalhada
- `PUT /api/v1/awards/ranking/{competition_id}/edit` - Editar
- `POST /api/v1/awards/ranking/{competition_id}/approve` - Aprovar

---

## üìä Database Schema

### Table: award_methodologies
```
PK: methodology_id
Attributes:
  - competition_name
  - criteria (JSON)
  - weights (JSON)
  - tiebreaker_rules
  - created_at
```

### Table: competition_rankings
```
PK: ranking_id
Attributes:
  - competition_id
  - methodology_id
  - groups_evaluated (JSON: [{group, scores, total}])
  - ranking_order (List)
  - explanations (JSON)
  - status (pending, approved, published)
  - created_at
```

---

## ü§ñ Agente CrewAI

```python
award_methodology_agent = Agent(
    role='Award Methodology Designer',
    goal='Create fair, transparent, and objective award criteria',
    backstory="""Expert in competition design and evaluation.
    You ensure that all participants understand why winners won
    and what they need to improve.""",
    tools=[CriteriaDesignTool(), FairnessCheckerTool()],
)
```

---

## ‚úÖ Crit√©rios de Aceita√ß√£o

- [ ] Metodologia clara e objetiva gerada
- [ ] Ranking automatizado com justificativas
- [ ] Explica√ß√£o do "o que faltou" para cada grupo
- [ ] Visualiza√ß√µes intuitivas
- [ ] Interface de aprova√ß√£o funcional
- [ ] Testes de fairness (sem vi√©s)
- [ ] Deploy serverless

---

## üìö Refer√™ncias

- [Competition Design Principles](https://www.kaggle.com/competitions)
- [Fairness in Ranking Systems](https://arxiv.org/abs/example)
- [Explainable AI](https://christophm.github.io/interpretable-ml-book/)
